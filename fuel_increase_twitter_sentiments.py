# -*- coding: utf-8 -*-
"""Fuel Increase Twitter Sentiments

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Au-84QhZOCphMVgzp4fxUYnHkwQRDY0T
"""

#pip install nltk==3.5

#!pip 

"""##Importing Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

##Ploting and Visualization
import seaborn as sns
import matplotlib.pyplot as plt
from IPython.display import display
from wordcloud import wordcloud
# %matplotlib inline

##Machine Learning and Feauture Selection
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

##Natural Language Toolkit
import nltk
from nltk.corpus import stopwords
from html.parser import HTMLParser
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import sent_tokenize, word_tokenize

##Model Evaluation
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score

##Read data
fuel=pd.read_csv("/content/scraped_tweets.csv")

##Show first 5 rows of dataframe
fuel.head()

fuel2=fuel.copy()

#Info
fuel.info()

#Basic stats
fuel.describe().T

#Duplication
fuel.duplicated()

#Missing values
fuel.isnull().sum(axis=1)

"""##Data Cleaning"""

import re
from textblob import TextBlob
from wordcloud import WordCloud
import matplotlib.pyplot as plt

#Create a function to clean the tweets
def cleanTxt(text):
 text=re.sub('@[A-Za-z0-9]+','',text) #Removing @mentions
 text=re.sub('#','',text) #Removing '#' hash tag
 text=re.sub('RT[\s]+','',text) #Removing RT
 text=re.sub('https?:\/\/\/S+','',text) #Removing hyperlinks

 return text

 #Clean the tweets
 fuel['text']=fuel['text'].apply(strip_all_entities)

 #Show clean tweets
 fuel

import re,string

def strip_links(text):
    link_regex    = re.compile('((https?):((//)|(\\\\))+([\w\d:#@%/;$()~_?\+-=\\\.&](#!)?)*)', re.DOTALL)
    links         = re.findall(link_regex, text)
    for link in links:
        text = text.replace(link[0], ', ')    
    return text

def strip_all_entities(text):
    entity_prefixes = ['@','#']
    for separator in  string.punctuation:
        if separator not in entity_prefixes :
            text = text.replace(separator,' ')
    words = []
    for word in text.split():
        word = word.strip()
        if word:
            if word[0] not in entity_prefixes:
                words.append(word)
    return ' '.join(words)

#Create a function to get subjectivity
def getSubjectivity(text):
  return TextBlob(text).sentiment.subjectivity

#Create a function to get the polarity
def getPolarity(text):
  return TextBlob(text).sentiment.polarity

#Create two new columns 'Subjectivity' & 'Polarity'
fuel['Subjectivity'] = fuel['text'].apply(getSubjectivity)
fuel['Polarity']=fuel['text'].apply(getPolarity)

#Show the new dataframe with columns 'Subjectivity' & 'Polarity'
fuel

#Changing tweets to lower case
fuel['clean_tweets']=fuel['text'].apply(lambda x:x.lower())
fuel.head(3)

##word tokenization
fuel['text_token']=fuel['clean_tweets'].apply(lambda x:word_tokenize(x))

#Fully formatted tweets & there tokens
fuel.head(3)

"""#Removing stop words"""

#Importing stop words from NLTK corpus for english language
stop_words =set(stopwords.words('english'))

#Created new columns of tokens without stopwords
fuel['text_token_filtered'] =fuel['text_token'].apply(lambda x:[word for word in x if not word in stop_words])

#Tokenized columns with stop words and without stop words
fuel[['text_token','text_token_filtered']].head(3)

"""#Word stemming"""

#Created one more columns tweet_stemmed
stemming=PorterStemmer()
fuel['text_stemmed']=fuel['text_token_filtered'].apply(lambda x: ' '.join([stemming.stem(i) for i in x]))
fuel.head(3)

"""#Exploratory data analysis"""

#Word cloud visualization
allWords=''.join([twts for twts in fuel['clean_tweets']])
wordCloud=WordCloud(width=500, height=300, random_state=21, max_font_size=110).generate(allWords)

plt.imshow(wordCloud,interpolation="bilinear")
plt.axis('off')
plt.show()

#Create a function to compute negative (-1), neutral (0) and positive (+1) analysis
def getAnalysis(score):
  if score < 0:
    return 'Negative'
  elif score == 0:
    return 'Neutral'
  else:
    return 'Positive'
    
fuel['Analysis'] =fuel['Polarity'].apply(getAnalysis)
# Show the dataframe
fuel

#Plotting and Visualizing 
plt.title('Sentiment Analysis')
plt.xlabel('Sentiment')
plt.ylabel('Counts')
fuel['Analysis'].value_counts().plot(kind = 'bar')
plt.show()

fuel.head(2)

#Create a Corpus for every class sentiment 
neutral_text = ' '.join([text for text in fuel['text']
                        [fuel['Analysis'] == 'Neutral']])
pro_text = ' '.join([text for text in fuel['text']
                       [fuel['Analysis'] == 'Positive']])
negative_text = ' '.join([text for text in fuel['text']
                           [fuel['Analysis'] == 'Negative']])

import nltk

#Visualising each sentiment class according to the count of words
full_title = ['Most Popular words for News tweets',
              'Most popular words for Pro tweets',
              'Most popular words for Neutral tweets',]



text_list = [ pro_text,
              neutral_text, negative_text]

plt.rcParams['figure.figsize'] = [50, 5]

for i, sent in enumerate(text_list):
    plt.subplot(1, 4, i + 1)
    freq_dist = nltk.FreqDist(sent.split(' '))
    df = pd.DataFrame({'Word': list(freq_dist.keys()),
                      'Count' : list(freq_dist.values())})

    df = df.nlargest(columns='Count', n=15)

    ax = sns.barplot(data=df, y='Word', x='Count')
    plt.title(full_title[i])
    plt.show()

# Create word clouds of the most common words in each sentiment class
wc = WordCloud(width=800, height=500, 
               background_color='white', colormap='Dark2',
               max_font_size=150, random_state=42)

plt.rcParams['figure.figsize'] = [20, 15]

# Create subplots 
for i in range(0, len(text_list)):
    wc.generate(text_list[1])
    
    plt.subplot(2, 2, i + 1)
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.title(full_title[i])
    
plt.show()

#Exploring the impact of Hashtags on the sentiment classes:

# Creating a function to extract hashtags from tweets
def extract_hashtags(x):
    """ The following function finds hashtags on a 
        tweet and returns them as a list"""
    hashtags = []
    for i in x:
        ht = re.findall(r'#(\w+)', i)
        hashtags.append(ht)

fuel2.head()

# Extracting hashtags from tweets
news_ht = extract_hashtags(fuel2['text']
                              [fuel2['sentiment'] == 2])
pro_ht = extract_hashtags(tweets_fuel['text']
                          [fuel2['sentiment'] == 1])
neutral_ht = extract_hashtags(fuel2['text']
                              [fuel2['sentiment'] == 0])
anti_ht = extract_hashtags(fuel2['message']
                          [fuel['sentiment'] == -1])

# Unnesting list
hashtags = [sum(news_ht, []), sum(pro_ht, []),
            sum(neutral_ht, []),sum(anti_ht, [])]

# Visualising the Hashtags
full_title = ['Impact of Hashtags on the News sentiment',
              'Impact of Hashtags on the Pro sentiment',
              'Impact of Hashtags on the Neutral sentiment',
              'Impact of Hashtags on the Anti sentiment']

plt.rcParams['figure.figsize'] = [50, 5]

for i, sent in enumerate(hashtags):
    plt.subplot(1, 4, i + 1)
    freq_dist = nltk.FreqDist(sent)
    df = pd.DataFrame({'Hashtag': list(freq_dist.keys()),
                      'Count' : list(freq_dist.values())})

    df = df.nlargest(columns='Count', n=15)

    ax = sns.barplot(data=df, y='Hashtag', x='Count')
    plt.title(full_title[i])
    plt.show()

"""#Feature Engineering"""

#Setting the parameters for the Vectorizer
vectorize=CountVectorizer(analyzer='word',
                          tokenizer=None,
                          preprocessor=None,
                          stop_words=None,
                          max_features=180000,
                          min_df=1,
                          ngram_range=(1,2)  
)

fuel.head(2)

#Create a function to compute negative (-1), neutral (0) and positive (+1) analysis
def getcat(score):
  if score == "Negative":
    return -1
  elif score == 'Neutral':
    return 0
  else:
    return 1
    
fuel['Sentiment'] =fuel['Analysis'].apply(getcat)
# Show the dataframe
fuel

clean_data=fuel[['clean_tweets','Sentiment']]

#Splitting the data into train & test
clean_data[:105]
test_stemmed=clean_data[:105].drop(['Sentiment'],axis=1)

fuel.reset_index(drop=True, inplace=True)

clean_data.reset_index(drop=True, inplace=True)

clean_data

X=clean_data.drop(['Sentiment'],axis=1)
y=clean_data['Sentiment']

clean_data.head(2)

train=fuel.copy()
train.drop(columns=['username','location'],inplace=True)
fuel

#Creating Bag Of Words
bow=CountVectorizer( min_df=2, max_features=1000)
bow.fit(fuel['text'])
bow_fuel=bow.transform(fuel['text']).toarray()
print('feature name==',bow.get_feature_names()[:10])
print('number of uniqe words',bow_fuel.shape[1])
print('shape',bow_fuel.shape)
bow_train=pd.DataFrame(bow_fuel)
bow_train['text']=fuel['text']
bow_train['text']=fuel['text']
bow_train.head()

#TF-IDF Features (Bi-Grams)

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf=TfidfVectorizer(ngram_range=(1, 2),min_df=2,max_features=1000)
tfidf.fit(fuel['text'])
tfidf_fuel=tfidf.transform(fuel['text']).toarray()
print('number of uniqe words',bow_fuel.shape[1])
print('shape',tfidf_fuel.shape)
tfidf_train=pd.DataFrame(tfidf_fuel)
tfidf_train['text']=fuel['text']
tfidf_train['text']=fuel['text']
tfidf_train.head()

#Word2vec
from gensim.models import Word2Vec
tokenize=fuel['text'].apply(lambda x: x.split())
w2vec_model=Word2Vec(tokenize,min_count = 1, size = 100, window = 5, sg = 1)
w2vec_model.train(tokenize,total_examples= len(fuel['text']),epochs=20)

w2vec_model.most_similar('Eskom')

w2v_words = list(w2vec_model.wv.vocab)
print("number of words that occured minimum 5 times ",len(w2v_words))
print("sample words ", w2v_words[0:50])

vector=[]
from tqdm import tqdm
for sent in tqdm(tokenize):
  sent_vec=np.zeros(100)
  count =0
  for word in sent: 
    if word in w2v_words:
      vec = w2vec_model.wv[word]
      sent_vec += vec 
      count += 1
  if count != 0:
    sent_vec /= count #normalize
  vector.append(sent_vec)
print(len(vector))
print(len(vector[0]))

#example
l='Eskom'
count=0
vcc=np.zeros(100)
for word in l:
  if word in w2v_words:
    v=w2vec_model.wv[word]
    vcc+=v
    count+=1
vcc

from sklearn.pipeline import Pipeline

#Building the pipeline
# Building a Pipeline for word vectorization
pipe = Pipeline( [('vect', CountVectorizer)] )

# Fitting & transforming the data
vect = pipe.fit_transform
print(vect)

# Inspecting the shape of our vectorized data
print('train dim:',vect)

"""#Splitting Dataset"""

# Splitting the data into train & test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( X,y, random_state=0, train_size=.75)
from sklearn.datasets import make_blobs
# create dataset
X, y = make_blobs(n_samples=100)
# split into train test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)
# summarize first 5 rows
print(X_train[:5, :])
# split again, and we should see the same split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)
# summarize first 5 rows
print(X_train[:5, :])

# get the locations
X = fuel.iloc[:, :-1]
y = fuel.iloc[:, -1]

"""#Building LSTM

"""

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

regressor = Sequential()

regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 50, return_sequences = True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 50, return_sequences = True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 50))
regressor.add(Dropout(0.2))

regressor.add(Dense(units = 1))

regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')

regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)

fuel2=fuel.copy()

fuel2.drop(['text','totaltweets'], axis=1)

totaltweets=fuel2.iloc[:, 1:2].values

totaltweets

import sklearn
from sklearn import linear_model

dataset_total = pd.concat((fuel['totaltweets'],fuel2['totaltweets']), axis = 0)
inputs = dataset_total[len(dataset_total) - len(fuel2) - 60:].values
inputs = inputs.reshape(-1,1)
X_test = []
for i in range(60, 76):
    X_test.append(inputs[i-60:i, 0])
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
totaltweets = regressor.predict(X_train)

"""#Downloading notebook"""

#from google.colab import files
#files.download('sample_data')